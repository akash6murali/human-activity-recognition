{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3fa304",
   "metadata": {},
   "source": [
    "### Run the notebook in the directory which contains capture24 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d516401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashmurali/anaconda3/envs/har/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9679f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akashmurali/Documents/capstone/project/capture24\n"
     ]
    }
   ],
   "source": [
    "cd capture24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping df\n",
    "annotation_df = pd.read_csv(\"annotation-label-dictionary.csv\", index_col =\"annotation\", dtype = \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ede9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each participants data into a df seperately\n",
    "def load_participant_df(filepath):\n",
    "    \n",
    "    df = pd.read_csv(filepath, index_col = \"time\", parse_dates =['time'], dtype = {'x':'f4', 'y':'f4', 'z':'f4', 'annotation':'string'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9d3cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map to a given schema\n",
    "def map_annotation(df, annotation_df, schemas):\n",
    "\n",
    "    for s in schemas:\n",
    "        label_col = f'label:{s}'\n",
    "        if label_col in annotation_df.columns:\n",
    "            label_map = annotation_df[label_col].to_dict()\n",
    "            df[s] = df['annotation'].map(label_map)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4089d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validity of the window\n",
    "def is_valid_window(window, excepected_length):\n",
    "    if len(window)!= excepected_length:\n",
    "        return False\n",
    "    if window[['x', 'y', 'z']].isna().any().any():\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49545116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_windows(df, schemas, window_size = 10, sample_rate = 100): # default to 10sec and 100hz\n",
    "    \n",
    "    X,Y,T = [],{schema:[] for schema in schemas}, []\n",
    "    excepected_length = window_size * sample_rate\n",
    "\n",
    "    for timestamp, window in df.resample(f'{window_size}s', origin = \"start\"):\n",
    "        if not is_valid_window(window, excepected_length):\n",
    "            continue\n",
    "        x = window[['x','y','z']].to_numpy()\n",
    "        labels_valid = True\n",
    "        window_labels = {}\n",
    "        for schema in schemas:\n",
    "            if schema not in window.columns:\n",
    "                labels_valid = False\n",
    "                break\n",
    "\n",
    "            label_mode = window[schema].mode(dropna = False)\n",
    "\n",
    "            if len(label_mode) == 0:\n",
    "                labels_valid = False\n",
    "                break\n",
    "            window_labels[schema] = label_mode.iloc[0]\n",
    "\n",
    "        if not labels_valid:\n",
    "            continue\n",
    "        X.append(x)\n",
    "        T.append(timestamp)\n",
    "\n",
    "        for schema in schemas:\n",
    "            Y[schema].append(window_labels[schema])\n",
    "    X = np.stack(X)\n",
    "    T = np.array(T)\n",
    "    for schema in schemas:\n",
    "        Y[schema] = np.array(Y[schema])\n",
    "\n",
    "    \n",
    "    return X,Y,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0caec274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "loading all participants data\n",
      "======================================================================\n",
      "total files found 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing participants: 100%|██████████| 151/151 [28:43<00:00, 11.41s/it]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_all(annotation_df, schemas, output_dir = \"preprocessed\", window_size = 10, sample_rate = 100):\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"loading all participants data\")\n",
    "    print(\"=\" * 70)\n",
    "    participant_files = sorted(glob.glob('P[0-9][0-9][0-9].csv*'))\n",
    "    print(\"total files found\",len(participant_files))\n",
    "\n",
    "    all_X = [] # all x,y,z acceleartion\n",
    "    all_Y = {schema:[] for schema in schemas} # Willets label map and Walmsley label map\n",
    "    all_T = [] # all timestamps\n",
    "    all_P = [] # all participants/test subjects\n",
    "\n",
    "    for filepath in tqdm(participant_files, desc = \"processing participants\"):\n",
    "        try:\n",
    "            p_id = filepath.split('.')[0]\n",
    "            df = load_participant_df(filepath)\n",
    "            df = map_annotation(df, annotation_df, schemas)\n",
    "            X,Y,T = extract_windows(df, schemas, window_size, sample_rate)\n",
    "            if X is None:\n",
    "                print(f\"no valid window for {p_id}\")\n",
    "                continue\n",
    "            all_X.append(X)\n",
    "            all_T.append(T)\n",
    "            all_P.append(np.array([p_id] * len(X)))\n",
    "\n",
    "            for schema in schemas:\n",
    "                all_Y[schema].append(Y[schema])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error processing {filepath}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # combining data from all participants\n",
    "    X_combined = np.vstack(all_X)\n",
    "    T_combined = np.hstack(all_T)\n",
    "    P_combined = np.hstack(all_P)\n",
    "    \n",
    "    Y_combined = {}\n",
    "    for schema in schemas:\n",
    "        Y_combined[schema] = np.hstack(all_Y[schema])\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    np.save(os.path.join(output_dir, 'X.npy'), X_combined)\n",
    "    np.save(os.path.join(output_dir, 'T.npy'), T_combined)\n",
    "    np.save(os.path.join(output_dir, 'P.npy'), P_combined)\n",
    "\n",
    "    for schema in schemas:\n",
    "        np.save(os.path.join(output_dir, f'Y_{schema}.npy'), Y_combined[schema])\n",
    "\n",
    "\n",
    "schemas = [\"WillettsSpecific2018\"]\n",
    "preprocess_all(annotation_df, schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4dd22",
   "metadata": {},
   "source": [
    "#### to load the processed .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b674756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prepared data from: preprocessed\n",
      "\n",
      "Loaded data:\n",
      "  X shape: (1398022, 1000, 3)\n",
      "  Y shape: (1398022,)\n",
      "  Number of participants: 151\n"
     ]
    }
   ],
   "source": [
    "def load_prepared_data(data_dir='preprocessed', schema='WillettsSpecific2018'):\n",
    "    print(f\"Loading prepared data from: {data_dir}\")\n",
    "    \n",
    "    X = np.load(os.path.join(data_dir, 'X.npy'))\n",
    "    Y = np.load(os.path.join(data_dir, f'Y_{schema}.npy'), allow_pickle=True)\n",
    "    T = np.load(os.path.join(data_dir, 'T.npy'), allow_pickle=True)\n",
    "    P = np.load(os.path.join(data_dir, 'P.npy'), allow_pickle=True)\n",
    "    \n",
    "    print(f\"\\nLoaded data:\")\n",
    "    print(f\"  X shape: {X.shape}\")\n",
    "    print(f\"  Y shape: {Y.shape}\")\n",
    "    print(f\"  Number of participants: {len(np.unique(P))}\")\n",
    "    \n",
    "    return X, Y, T, P\n",
    "\n",
    "\n",
    "# Usage\n",
    "X, Y, T, P = load_prepared_data(schema='WillettsSpecific2018')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "har",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
