{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142085f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prepared data from: preprocessed\n",
      "\n",
      "Loaded data:\n",
      "  X shape: (1398022, 1000, 3)\n",
      "  Y shape: (1398022,)\n",
      "  Number of participants: 151\n",
      "Extracting 32 features from 1398022 windows...\n",
      "Features: ['avg', 'std', 'skew', 'kurt', 'min', 'q25', 'med', 'q75', 'max', 'acf_1st_max', 'acf_1st_max_loc', 'acf_1st_min', 'acf_1st_min_loc', 'acf_zeros', 'pentropy', 'power', 'f1', 'f2', 'f3', 'p1', 'p2', 'p3', 'fft0', 'fft1', 'fft2', 'fft3', 'fft4', 'fft5', 'npeaks', 'peaks_avg_promin', 'peaks_min_promin', 'peaks_max_promin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1399/1399 [30:12<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete!\n",
      "Output shape: (1398022, 32)\n",
      "Features extracted: ['avg', 'std', 'skew', 'kurt', 'min', 'q25', 'med', 'q75', 'max', 'acf_1st_max', 'acf_1st_max_loc', 'acf_1st_min', 'acf_1st_min_loc', 'acf_zeros', 'pentropy', 'power', 'f1', 'f2', 'f3', 'p1', 'p2', 'p3', 'fft0', 'fft1', 'fft2', 'fft3', 'fft4', 'fft5', 'npeaks', 'peaks_avg_promin', 'peaks_min_promin', 'peaks_max_promin']\n",
      "\n",
      "Feature statistics:\n",
      "  Min values: [ -0.1642207    0.         -16.381773    -1.9261225   -0.98326266]...\n",
      "  Max values: [1.7581493e+00 1.0460494e+00 2.8646334e+01 8.7671014e+02 6.9947183e-01]...\n",
      "  Mean values: [ 0.01215498  0.06573144  0.53021324  5.2717385  -0.1811054 ]...\n",
      "  NaN count: 0\n",
      "  Inf count: 0\n",
      "\n",
      "Saved features to: preprocessed/X_features.npy\n",
      "Saved feature names to: preprocessed/feature_names.npy\n",
      "\n",
      "==================================================\n",
      "Feature extraction completed successfully!\n",
      "Original data shape: (1398022, 1000, 3)\n",
      "Feature matrix shape: (1398022, 32)\n",
      "Ready for ML model training!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "from scipy.ndimage import median_filter\n",
    "import statsmodels.tsa.stattools as stattools\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "MIN_WINDOW_SEC = 2  \n",
    "\n",
    "def extract_features(xyz, sample_rate=100):\n",
    "    '''Extract commonly used HAR time-series features. xyz is a window of shape (N,3)'''\n",
    "    \n",
    "    if np.isnan(xyz).any():\n",
    "        return {}\n",
    "    \n",
    "    if len(xyz) <= MIN_WINDOW_SEC * sample_rate:\n",
    "        return {}\n",
    "    \n",
    "    feats = {}\n",
    "    \n",
    "    v = np.linalg.norm(xyz, axis=1)\n",
    "    v = median_filter(v, size=5, mode='nearest')\n",
    "    v = v - 1  # detrend: \"remove gravity\"\n",
    "    v = np.clip(v, -2, 2)  # clip abnormaly high values\n",
    "    \n",
    "    # Moments features\n",
    "    feats.update(moments_features(v, sample_rate))\n",
    "    \n",
    "    # Quantile features\n",
    "    feats.update(quantile_features(v, sample_rate))\n",
    "    \n",
    "    # Autocorrelation features\n",
    "    feats.update(autocorr_features(v, sample_rate))\n",
    "    \n",
    "    # Spectral features\n",
    "    feats.update(spectral_features(v, sample_rate))\n",
    "    \n",
    "    # FFT features\n",
    "    feats.update(fft_features(v, sample_rate))\n",
    "    \n",
    "    # Peak features\n",
    "    feats.update(peaks_features(v, sample_rate))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def moments_features(v, sample_rate=None):\n",
    "    \"\"\"Moments\"\"\"\n",
    "    avg = np.mean(v)\n",
    "    std = np.std(v)\n",
    "    if std > .01:\n",
    "        skew = np.nan_to_num(stats.skew(v))\n",
    "        kurt = np.nan_to_num(stats.kurtosis(v))\n",
    "    else:\n",
    "        skew = kurt = 0\n",
    "    feats = {\n",
    "        'avg': avg,\n",
    "        'std': std,\n",
    "        'skew': skew,\n",
    "        'kurt': kurt,\n",
    "    }\n",
    "    return feats\n",
    "\n",
    "\n",
    "def quantile_features(v, sample_rate=None):\n",
    "    \"\"\"Quantiles (min, 25th, med, 75th, max)\"\"\"\n",
    "    feats = {}\n",
    "    feats['min'], feats['q25'], feats['med'], feats['q75'], feats['max'] = np.quantile(v, (0, .25, .5, .75, 1))\n",
    "    return feats\n",
    "\n",
    "\n",
    "def autocorr_features(v, sample_rate):\n",
    "    \"\"\"Autocorrelation features\"\"\"\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # ignore invalid div warnings\n",
    "        u = np.nan_to_num(stattools.acf(v, nlags=2 * sample_rate))\n",
    "    \n",
    "    peaks, _ = signal.find_peaks(u, prominence=.1)\n",
    "    if len(peaks) > 0:\n",
    "        acf_1st_max_loc = peaks[0]\n",
    "        acf_1st_max = u[acf_1st_max_loc]\n",
    "        acf_1st_max_loc /= sample_rate  # in secs\n",
    "    else:\n",
    "        acf_1st_max = acf_1st_max_loc = 0.0\n",
    "    \n",
    "    valleys, _ = signal.find_peaks(-u, prominence=.1)\n",
    "    if len(valleys) > 0:\n",
    "        acf_1st_min_loc = valleys[0]\n",
    "        acf_1st_min = u[acf_1st_min_loc]\n",
    "        acf_1st_min_loc /= sample_rate  # in secs\n",
    "    else:\n",
    "        acf_1st_min = acf_1st_min_loc = 0.0\n",
    "    \n",
    "    acf_zeros = np.sum(np.diff(np.signbit(u)))\n",
    "    \n",
    "    feats = {\n",
    "        'acf_1st_max': acf_1st_max,\n",
    "        'acf_1st_max_loc': acf_1st_max_loc,\n",
    "        'acf_1st_min': acf_1st_min,\n",
    "        'acf_1st_min_loc': acf_1st_min_loc,\n",
    "        'acf_zeros': acf_zeros,\n",
    "    }\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def spectral_features(v, sample_rate):\n",
    "    \"\"\"Spectral entropy, average power, dominant frequencies\"\"\"\n",
    "    \n",
    "    feats = {}\n",
    "    \n",
    "    freqs, powers = signal.periodogram(v, fs=sample_rate, detrend='constant', scaling='density')\n",
    "    powers /= (len(v) / sample_rate)    # unit/sec\n",
    "    \n",
    "    feats['pentropy'] = stats.entropy(powers[powers > 0])\n",
    "    feats['power'] = np.sum(powers)\n",
    "    \n",
    "    peaks, _ = signal.find_peaks(powers)\n",
    "    peak_powers = powers[peaks]\n",
    "    peak_freqs = freqs[peaks]\n",
    "    peak_ranks = np.argsort(peak_powers)[::-1]\n",
    "    \n",
    "    TOPN = 3\n",
    "    feats.update({f\"f{i + 1}\": 0 for i in range(TOPN)})\n",
    "    feats.update({f\"p{i + 1}\": 0 for i in range(TOPN)})\n",
    "    for i, j in enumerate(peak_ranks[:TOPN]):\n",
    "        feats[f\"f{i + 1}\"] = peak_freqs[j]\n",
    "        feats[f\"p{i + 1}\"] = peak_powers[j]\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def fft_features(v, sample_rate, nfreqs=5):\n",
    "    \"\"\"Power of frequencies 0Hz, 1Hz, 2Hz, ... using Welch's method\"\"\"\n",
    "    \n",
    "    _, powers = signal.welch(\n",
    "        v, fs=sample_rate,\n",
    "        nperseg=sample_rate,\n",
    "        noverlap=sample_rate // 2,\n",
    "        detrend='constant',\n",
    "        scaling='density',\n",
    "        average='median'\n",
    "    )\n",
    "    \n",
    "    feats = {f\"fft{i}\": powers[i] for i in range(nfreqs + 1)}\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def peaks_features(v, sample_rate):\n",
    "    \"\"\"Features of the signal peaks\"\"\"\n",
    "    \n",
    "    feats = {}\n",
    "    u = butterfilt(v, 5, fs=sample_rate)  # lowpass 5Hz\n",
    "    peaks, peak_props = signal.find_peaks(u, distance=0.2 * sample_rate, prominence=0.25)\n",
    "    feats['npeaks'] = len(peaks) / (len(v) / sample_rate)  # peaks/sec\n",
    "    if len(peak_props['prominences']) > 0:\n",
    "        feats['peaks_avg_promin'] = np.mean(peak_props['prominences'])\n",
    "        feats['peaks_min_promin'] = np.min(peak_props['prominences'])\n",
    "        feats['peaks_max_promin'] = np.max(peak_props['prominences'])\n",
    "    else:\n",
    "        feats['peaks_avg_promin'] = feats['peaks_min_promin'] = feats['peaks_max_promin'] = 0\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def butterfilt(x, cutoffs, fs, order=4, axis=0):\n",
    "    \"\"\"Butterworth filter\"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    if isinstance(cutoffs, tuple):\n",
    "        hicut, lowcut = cutoffs\n",
    "        if hicut > 0:\n",
    "            btype = 'bandpass'\n",
    "            Wn = (hicut / nyq, lowcut / nyq)\n",
    "        else:\n",
    "            btype = 'low'\n",
    "            Wn = lowcut / nyq\n",
    "    else:\n",
    "        btype = 'low'\n",
    "        Wn = cutoffs / nyq\n",
    "    sos = signal.butter(order, Wn, btype=btype, analog=False, output='sos')\n",
    "    y = signal.sosfiltfilt(sos, x, axis=axis)\n",
    "    return y\n",
    "\n",
    "\n",
    "def get_feature_names():\n",
    "    \"\"\"Get the list of feature names in consistent order\"\"\"\n",
    "    # Create a dummy window to extract feature names\n",
    "    dummy_window = np.random.randn(500, 3) * 0.1\n",
    "    feats = extract_features(dummy_window, 100)\n",
    "    return list(feats.keys())\n",
    "\n",
    "\n",
    "def extract_features_batch(X, sample_rate=100, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Extract features from all windows in X\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array of shape (n_samples, n_timesteps, 3)\n",
    "        The input accelerometer data\n",
    "    sample_rate : int\n",
    "        Sampling rate in Hz (default 100)\n",
    "    batch_size : int\n",
    "        Number of windows to process at once for progress updates\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_features : numpy array of shape (n_samples, 32)\n",
    "        Extracted features for all windows\n",
    "    feature_names : list\n",
    "        Names of the features in order\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Get feature names from a dummy extraction\n",
    "    feature_names = get_feature_names()\n",
    "    n_features = len(feature_names)\n",
    "    \n",
    "    print(f\"Extracting {n_features} features from {n_samples} windows...\")\n",
    "    print(f\"Features: {feature_names}\")\n",
    "    \n",
    "    # Initialize feature matrix\n",
    "    X_features = np.zeros((n_samples, n_features), dtype=np.float32)\n",
    "    \n",
    "    # Track failed extractions\n",
    "    failed_indices = []\n",
    "    \n",
    "    # Process in batches for progress tracking\n",
    "    for batch_start in tqdm(range(0, n_samples, batch_size), desc=\"Processing batches\"):\n",
    "        batch_end = min(batch_start + batch_size, n_samples)\n",
    "        \n",
    "        for i in range(batch_start, batch_end):\n",
    "            try:\n",
    "                # Extract features for this window\n",
    "                features = extract_features(X[i], sample_rate)\n",
    "                \n",
    "                if features:  # If extraction was successful\n",
    "                    # Fill in the feature values in consistent order\n",
    "                    for j, feat_name in enumerate(feature_names):\n",
    "                        X_features[i, j] = features.get(feat_name, 0.0)\n",
    "                else:\n",
    "                    # Mark as failed extraction\n",
    "                    failed_indices.append(i)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing window {i}: {e}\")\n",
    "                failed_indices.append(i)\n",
    "    \n",
    "    if failed_indices:\n",
    "        print(f\"\\nWarning: {len(failed_indices)} windows failed feature extraction\")\n",
    "        print(f\"Failed indices: {failed_indices[:10]}...\" if len(failed_indices) > 10 else f\"Failed indices: {failed_indices}\")\n",
    "    \n",
    "    print(f\"\\nFeature extraction complete!\")\n",
    "    print(f\"Output shape: {X_features.shape}\")\n",
    "    print(f\"Features extracted: {feature_names}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(f\"  Min values: {np.min(X_features, axis=0)[:5]}...\")\n",
    "    print(f\"  Max values: {np.max(X_features, axis=0)[:5]}...\")\n",
    "    print(f\"  Mean values: {np.mean(X_features, axis=0)[:5]}...\")\n",
    "    print(f\"  NaN count: {np.isnan(X_features).sum()}\")\n",
    "    print(f\"  Inf count: {np.isinf(X_features).sum()}\")\n",
    "    \n",
    "    return X_features, feature_names\n",
    "\n",
    "\n",
    "def load_prepared_data(data_dir='preprocessed', schema='WillettsSpecific2018'):\n",
    "    \"\"\"Load your preprocessed data\"\"\"\n",
    "    print(f\"Loading prepared data from: {data_dir}\")\n",
    "    \n",
    "    X = np.load(os.path.join(data_dir, 'X.npy'))\n",
    "    Y = np.load(os.path.join(data_dir, f'Y_{schema}.npy'), allow_pickle=True)\n",
    "    T = np.load(os.path.join(data_dir, 'T.npy'), allow_pickle=True)\n",
    "    P = np.load(os.path.join(data_dir, 'P.npy'), allow_pickle=True)\n",
    "    \n",
    "    print(f\"\\nLoaded data:\")\n",
    "    print(f\"  X shape: {X.shape}\")\n",
    "    print(f\"  Y shape: {Y.shape}\")\n",
    "    print(f\"  Number of participants: {len(np.unique(P))}\")\n",
    "    \n",
    "    return X, Y, T, P\n",
    "\n",
    "\n",
    "def save_features(X_features, feature_names, save_dir='preprocessed'):\n",
    "    \"\"\"Save extracted features and feature names\"\"\"\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save features\n",
    "    features_path = os.path.join(save_dir, 'X_features.npy')\n",
    "    np.save(features_path, X_features)\n",
    "    print(f\"\\nSaved features to: {features_path}\")\n",
    "    \n",
    "    # Save feature names\n",
    "    names_path = os.path.join(save_dir, 'feature_names.npy')\n",
    "    np.save(names_path, np.array(feature_names))\n",
    "    print(f\"Saved feature names to: {names_path}\")\n",
    "    \n",
    "    return features_path, names_path\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Load your data\n",
    "    X, Y, T, P = load_prepared_data(schema='WillettsSpecific2018')\n",
    "    \n",
    "    # Extract features from all windows\n",
    "    # Assuming 100Hz sample rate (1000 timesteps = 10 seconds at 100Hz)\n",
    "    X_features, feature_names = extract_features_batch(X, sample_rate=100, batch_size=1000)\n",
    "    \n",
    "    # Save the features\n",
    "    save_features(X_features, feature_names)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Feature extraction completed successfully!\")\n",
    "    print(f\"Original data shape: {X.shape}\")\n",
    "    print(f\"Feature matrix shape: {X_features.shape}\")\n",
    "    print(f\"Ready for ML model training!\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99118178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Features shape: (1398022, 32)\n",
      "Labels shape: (1398022,)\n",
      "Label type: <U32\n",
      "Number of unique classes: 11\n",
      "Classes: ['bicycling' 'household-chores' 'manual-work' 'mixed-activity' 'nan'\n",
      " 'sitting' 'sleep' 'sports' 'standing' 'vehicle' 'walking']\n",
      "\n",
      "Handling labels...\n",
      "Removed 475823 samples with 'nan' labels\n",
      "Remaining samples: 922199\n",
      "\n",
      "Label mapping:\n",
      "  0: bicycling\n",
      "  1: household-chores\n",
      "  2: manual-work\n",
      "  3: mixed-activity\n",
      "  4: sitting\n",
      "  5: sleep\n",
      "  6: sports\n",
      "  7: standing\n",
      "  8: vehicle\n",
      "  9: walking\n",
      "\n",
      "Splitting data...\n",
      "Training set: (737759, 32)\n",
      "Test set: (184440, 32)\n",
      "\n",
      "Training XGBoost model...\n",
      "Training complete!\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "Calculating metrics...\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy:  0.7161\n",
      "\n",
      "Macro Averages (treats all classes equally):\n",
      "F1 Score:  0.4636\n",
      "Precision: 0.6359\n",
      "Recall:    0.4384\n",
      "\n",
      "Weighted Averages (weighted by class frequency):\n",
      "F1 Score:  0.6936\n",
      "Precision: 0.7117\n",
      "Recall:    0.7161\n",
      "==================================================\n",
      "\n",
      "Per-class Performance:\n",
      "  bicycling           : 0.717\n",
      "  household-chores    : 0.560\n",
      "  manual-work         : 0.024\n",
      "  mixed-activity      : 0.210\n",
      "  sitting             : 0.810\n",
      "  sleep               : 0.894\n",
      "  sports              : 0.406\n",
      "  standing            : 0.003\n",
      "  vehicle             : 0.387\n",
      "  walking             : 0.372\n",
      "\n",
      "Done! Model trained and evaluated.\n",
      "Test set accuracy: 71.61%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_data_and_features(data_dir='preprocessed', schema='WillettsSpecific2018'):\n",
    "    \"\"\"Load features and labels\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load features\n",
    "    X_features = np.load(os.path.join(data_dir, 'X_features.npy'))\n",
    "    \n",
    "    # Load labels\n",
    "    Y = np.load(os.path.join(data_dir, f'Y_{schema}.npy'), allow_pickle=True)\n",
    "    \n",
    "    print(f\"Features shape: {X_features.shape}\")\n",
    "    print(f\"Labels shape: {Y.shape}\")\n",
    "    print(f\"Label type: {Y.dtype}\")\n",
    "    print(f\"Number of unique classes: {len(np.unique(Y))}\")\n",
    "    print(f\"Classes: {np.unique(Y)}\")\n",
    "    \n",
    "    return X_features, Y\n",
    "\n",
    "def train_xgboost_model(X_features, Y, test_size=0.2, random_state=42):\n",
    "    \"\"\"Train a simple XGBoost model\"\"\"\n",
    "    \n",
    "    # Handle 'nan' labels - remove them\n",
    "    print(\"\\nHandling labels...\")\n",
    "    valid_indices = Y != 'nan'\n",
    "    X_features = X_features[valid_indices]\n",
    "    Y = Y[valid_indices]\n",
    "    print(f\"Removed {(~valid_indices).sum()} samples with 'nan' labels\")\n",
    "    print(f\"Remaining samples: {len(Y)}\")\n",
    "    \n",
    "    # Encode string labels to integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_encoded = label_encoder.fit_transform(Y)\n",
    "    \n",
    "    print(f\"\\nLabel mapping:\")\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        print(f\"  {i}: {label}\")\n",
    "    \n",
    "    # Train test split\n",
    "    print(\"\\nSplitting data...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_features, Y_encoded, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=Y_encoded  # Maintain class distribution\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Initialize XGBoost classifier with basic parameters\n",
    "    print(\"\\nTraining XGBoost model...\")\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob',\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate f1, precision, recall with macro average for multi-class\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Also calculate weighted averages (weighted by support)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL PERFORMANCE\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(\"\\nMacro Averages (treats all classes equally):\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(\"\\nWeighted Averages (weighted by class frequency):\")\n",
    "    print(f\"F1 Score:  {f1_weighted:.4f}\")\n",
    "    print(f\"Precision: {precision_weighted:.4f}\")\n",
    "    print(f\"Recall:    {recall_weighted:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Per-class performance\n",
    "    print(\"\\nPer-class Performance:\")\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        mask = y_test == i\n",
    "        if mask.sum() > 0:\n",
    "            class_acc = accuracy_score(y_test[mask], y_pred[mask])\n",
    "            print(f\"  {label:20s}: {class_acc:.3f}\")\n",
    "    \n",
    "    return model, label_encoder, {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1,\n",
    "        'precision_macro': precision,\n",
    "        'recall_macro': recall,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Load features and labels\n",
    "    X_features, Y = load_data_and_features()\n",
    "    \n",
    "    # Train model and get metrics\n",
    "    model, label_encoder, metrics = train_xgboost_model(X_features, Y)\n",
    "    \n",
    "    print(\"\\nDone! Model trained and evaluated.\")\n",
    "    print(f\"Test set accuracy: {metrics['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b3e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
